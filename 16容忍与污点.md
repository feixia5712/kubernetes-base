## Taint和Toleration

### 概念

```
Taint在一类服务器上打上污点，让不能容忍这个污点的Pod不能部署在打了污点的服务器上。Toleration是让Pod容忍节点上配置的污点，可以让一些需要特殊配置的Pod能够调用到具有污点和特殊配置的节点上。每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod，是不会被该节点接受的
```

### 创建污点

```
创建一个污点（一个节点可以有多个污点）：
	kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT
比如：
	kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule
```

- NoSchedule：禁止调度到该节点，已经在该节点上的Pod不受影响

- NoExecute：禁止调度到该节点，如果不符合这个污点，会立马被驱逐（或在一段时间后）

- PreferNoSchedule：尽量避免将Pod调度到指定的节点上，如果没有更合适的节点，可以部署到该节点

### Toleration配置

```
方式一完全匹配：
tolerations:
- key: "taintKey"
  operator: "Equal"
  value: "taintValue"
  effect: "NoSchedule"

方式二不完全匹配：
tolerations:
- key: "taintKey"
  operator: "Exists"
  effect: "NoSchedule"

如果 operator 是 Exists （此时容忍度不能指定 value），或者
如果 operator 是 Equal ，则它们的 value 应该相等

方式三大范围匹配（不推荐key为内置Taint）：
- key: "taintKey"
  operator: "Exists"

方式四匹配所有（不推荐）：
tolerations:
- operator: "Exists"
```

其他说明

```
停留时间配置：
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600  默认是300s
```

### 内置污点

```
node.kubernetes.io/not-ready：节点未准备好，相当于节点状态Ready的值为False。
node.kubernetes.io/unreachable：Node Controller访问不到节点，相当于节点状态Ready的值为Unknown。node.kubernetes.io/out-of-disk：节点磁盘耗尽。
node.kubernetes.io/memory-pressure：节点存在内存压力。
node.kubernetes.io/disk-pressure：节点存在磁盘压力。
node.kubernetes.io/network-unavailable：节点网络不可达。
node.kubernetes.io/unschedulable：节点不可调度。
node.cloudprovider.kubernetes.io/uninitialized：如果Kubelet启动时指定了一个外部的cloudprovider，它将给当前节点添加一个Taint将其标记为不可用。在cloud-controller-manager的一个controller初始化这个节点后，Kubelet将删除这个Taint。
```

### 案例

```
先打污点
kubectl taint node k8s-node01 ssd=true:NoSchedule
打标签,主要是为了nodeSelector
kubectl label node k8s-node01 ssd=true
创建pod，只会在有污点的机器上创建
[root@k8s-master01 tora]# cat toleration.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: nginx02
  labels:
    env: test
spec:
  containers:
  - name: nginx02
    image: nginx:latest
    imagePullPolicy: IfNotPresent
  nodeSelector:
    ssd: "true"
  tolerations:
  - key: "ssd"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule" 

查看
[root@k8s-master01 tora]# kubectl get -f toleration.yaml -owide
NAME      READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
nginx02   1/1     Running   0          7m13s   172.17.125.4   k8s-node01   <none>           <none>
```

#### **说明：**

- Kubernetes 会自动给 Pod 添加一个 key 为 `node.kubernetes.io/not-ready` 的容忍度 并配置 `tolerationSeconds=300`，除非用户提供的 Pod 配置中已经已存在了 key 为 `node.kubernetes.io/not-ready` 的容忍度。

- 同样，Kubernetes 会给 Pod 添加一个 key 为 `node.kubernetes.io/unreachable` 的容忍度 并配置 `tolerationSeconds=300`，除非用户提供的 Pod 配置中已经已存在了 key 为 `node.kubernetes.io/unreachable` 的容忍度。

- 这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod 默认能够继续停留在当前节点运行 5 分钟。

- [DaemonSet](https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/) 中的 Pod 被创建时， 针对以下污点自动添加的 `NoExecute` 的容忍度将不会指定 `tolerationSeconds`：

  - `node.kubernetes.io/unreachable`
  - `node.kubernetes.io/not-ready`

  这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。

查看

```
[root@k8s-master01 tora]# kubectl get pod nginx02 -oyaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-09-20T13:17:42Z"
  labels:
    env: test
  name: nginx02
  namespace: default
  resourceVersion: "3366064"
  uid: 0ba13f2f-0743-46c3-b41a-a5cd35974ab6
spec:
  containers:
  - image: nginx:latest
    imagePullPolicy: IfNotPresent
    name: nginx02
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-24gl7
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-node01
  nodeSelector:
    ssd: "true"
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoSchedule
    key: ssd
    operator: Equal
    value: "true"
  - effect: NoExecute
    key: node.kubernetes.io/not-ready  #默认添加
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable  ##默认添加
    operator: Exists
    tolerationSeconds: 300
  volumes:
.........还有未列出
```

说明补充

```
ls -l /usr/lib/systemd/system/kube-controller-manager.service
node节点挂掉之后,被标记为NotReady时间是kube-controller-manager的参数node-monitor-grace-period=40s控制的
```

### Taint命令总结

```
创建一个污点（一个节点可以有多个污点）：
	kubectl taint nodes NODE_NAME TAINT_KEY=TAINT_VALUE:EFFECT
比如：
	kubectl taint nodes k8s-node01 ssd=true:PreferNoSchedule
查看一个节点的污点：
	kubectl  get node k8s-node01 -o go-template --template {{.spec.taints}}
	kubectl describe node k8s-node01 | grep Taints -A 10
删除污点（和label类似）：
	基于Key删除： kubectl  taint nodes k8s-node01 ssd-
	基于Key+Effect删除： kubectl  taint nodes k8s-node01 ssd:PreferNoSchedule-
修改污点（Key和Effect相同）：
	 kubectl  taint nodes k8s-node01 ssd=true:PreferNoSchedule --overwrite
```

高级用法参考  节点压力驱逐https://kubernetes.io/zh/docs/concepts/scheduling-eviction/node-pressure-eviction/



