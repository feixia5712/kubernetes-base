## 中间件容器化

### K8s如何管理中间件集群

```
包管理工具：一句话总结功能就是可以很方便管理一些比较复杂的应用，比如MySQL集群、Redis集群等，可以一键式创建集群、扩容、备份等。常用的两种包管理工具是Operator和Helm。

Helm：更倾向于无状态应用的部署，比如公司的服务、某些不需要持久化数据的中间件、不需要实现额外功能的服务，比如备份、回滚等。
Operator：管理更为复杂的有状态服务，比如MySQL集群、Redis集群、Rook等。并且可以利用Operator实现扩容、备份、回滚等功能
```

### Operator

```
Operator------->创建Operator控制器------->创建自定义资源------------>执行相关逻辑
```

### 使用Operator创建Redis集群

```
Operator模板：https://github.com/operator-framework/awesome-operators
Redis Cluster Operator： https://github.com/ucloud/redis-cluster-operator
下载的官方网站https://operatorhub.io/
```

本次redis集群搭建主要参考https://github.com/ucloud/redis-cluster-operator

涉及operator自定义crd

```
1、创建Operator
先克隆代码
git clone https://github.com/ucloud/redis-cluster-operator.git
创建自定义crd
kubectl create -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml
查看
[root@k8s-master01 redis-cluster-operator]# kubectl api-resources|grep distributedredisclusters
distributedredisclusters          drc          redis.kun/v1alpha1                     true         DistributedRedisCluster
kubectl create -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml
创建集群
kubectl create ns redis-cluster
kubectl create -f deploy/service_account.yaml -n redis-cluster
kubectl create -f deploy/namespace/role.yaml -n redis-cluster
kubectl create -f deploy/namespace/role_binding.yaml -n redis-cluster
kubectl create -f deploy/namespace/operator.yaml -n redis-cluster

2、创建Redis集群
	Namespace级别的需要更改配置：
	# if your operator run as cluster-scoped, add this annotations
	# redis.kun/scope: cluster-scoped
	kubectl create -f deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml  -n redis-cluster
	或者执行下边的，有资源控制的限制
	【可选】提示：如果集群规模不大，资源少，可以自定义资源，把请求的资源降低
kubectl create -f  deploy/example/custom-resources.yaml  -n redis-cluster

3、查看集群状态
	[root@k8s-master01 ~]# kubectl get distributedrediscluster -n redis-cluster
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   3            Healthy   22h
变成healthy就是正常了
查看所有的
[root@k8s-master01 zookeeper]# kubectl get all -l redis.kun/name=example-distributedrediscluster -n redis-cluster
NAME                                          READY   STATUS    RESTARTS   AGE
pod/drc-example-distributedrediscluster-0-0   1/1     Running   0          23h
pod/drc-example-distributedrediscluster-0-1   1/1     Running   0          23h
pod/drc-example-distributedrediscluster-1-0   1/1     Running   0          23h
pod/drc-example-distributedrediscluster-1-1   1/1     Running   0          23h
pod/drc-example-distributedrediscluster-2-0   1/1     Running   0          23h
pod/drc-example-distributedrediscluster-2-1   1/1     Running   0          23h

NAME                                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE
service/example-distributedrediscluster     ClusterIP   10.96.171.86   <none>        6379/TCP,16379/TCP   23h
service/example-distributedrediscluster-0   ClusterIP   None           <none>        6379/TCP,16379/TCP   23h
service/example-distributedrediscluster-1   ClusterIP   None           <none>        6379/TCP,16379/TCP   23h
service/example-distributedrediscluster-2   ClusterIP   None           <none>        6379/TCP,16379/TCP   23h

NAME                                                     READY   AGE
statefulset.apps/drc-example-distributedrediscluster-0   2/2     23h
statefulset.apps/drc-example-distributedrediscluster-1   2/2     23h
statefulset.apps/drc-example-distributedrediscluster-2   2/2     23h

```

访问

```
[root@k8s-master01 zookeeper]# kubectl get svc -n redis-cluster
NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)              AGE
example-distributedrediscluster     ClusterIP   10.96.171.86    <none>        6379/TCP,16379/TCP   23h
example-distributedrediscluster-0   ClusterIP   None            <none>        6379/TCP,16379/TCP   23h
example-distributedrediscluster-1   ClusterIP   None            <none>        6379/TCP,16379/TCP   23h
example-distributedrediscluster-2   ClusterIP   None            <none>        6379/TCP,16379/TCP   23h
redis-cluster-operator-metrics      ClusterIP   10.96.194.236   <none>        8383/TCP,8686/TCP    24h
第一种方式服务器内部使用redis-cli访问
第二种方式借助pod里面的命令redis-cli
[root@k8s-master01 zookeeper]# kubectl get pod -n redis-cluster
NAME                                      READY   STATUS    RESTARTS   AGE
drc-example-distributedrediscluster-0-0   1/1     Running   0          23h
drc-example-distributedrediscluster-0-1   1/1     Running   0          23h
drc-example-distributedrediscluster-1-0   1/1     Running   0          23h
drc-example-distributedrediscluster-1-1   1/1     Running   0          23h
drc-example-distributedrediscluster-2-0   1/1     Running   0          23h
drc-example-distributedrediscluster-2-1   1/1     Running   0          23h
redis-cluster-operator-675ccbc697-f4zdp   1/1     Running   4          24h
[root@k8s-master01 zookeeper]# kubectl exec drc-example-distributedrediscluster-0-0 -n redis-cluster -it -- sh
/data # redis-cli -h example-distributedrediscluster.redis-cluster
example-distributedrediscluster.redis-cluster:6379> info
# Server
redis_version:5.0.4
redis_git_sha1:00000000
redis_git_dirty:0
目前来说这个redis集群还不能够在生产环境直接使用，还需要改进
```

#### 扩容

```
[root@k8s-master01 redis-cluster-operator]# kubectl get distributedrediscluster -n redis-cluster
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   3            Healthy   23h
扩容直接修改
kubectl edit distributedrediscluster -n redis-cluster
```

### 使用Helm创建Kafka、Zookeeper集群

#### helm的安装以及源配置

```
查看helm支持的版本号与k8s匹配
https://helm.sh/zh/docs/topics/version_skew/
安装参考：https://helm.sh/zh/docs/intro/install/
下载 需要的版本 :http://github.com/helm/helm/releases
解压(tar -zxvf helm-v3.x.x-linux-amd64.tar.gz)
在解压目中找到helm程序，移动到需要的目录中(mv linux-amd64/helm /usr/local/bin/helm)
查看版本
[root@k8s-master01 ~]# helm version
version.BuildInfo{Version:"v3.6.3", GitCommit:"d506314abfb5d21419df8c7e7e68012379db2354", GitTreeState:"clean", GoVersion:"go1.16.5"}
```

添加源

```
添加bitnami和官方helm仓库：
	helm repo add bitnami https://charts.bitnami.com/bitnami
	helm repo add stable https://charts.helm.sh/stable
```

#### 使用Helm创建Kafka、Zookeeper集群

- 先下载后安装

  ```
  查询包
  [root@k8s-master01 ~]# helm search repo  zookeeper 
  NAME                    	CHART VERSION	APP VERSION	DESCRIPTION                                       
  bitnami/zookeeper       	7.4.10       	3.7.0      	A centralized service for maintaining configura...
  bitnami/dataplatform-bp1	8.0.2        	0.0.11     	OCTO Data platform Kafka-Spark-Solr Helm Chart    
  bitnami/dataplatform-bp2	8.0.5        	0.0.11     	OCTO Data platform Kafka-Spark-Elasticsearch He...
  bitnami/kafka           	14.2.4       	2.8.1      	Apache Kafka is a distributed streaming platform. 
  bitnami/solr            	2.1.1        	8.10.1     	Apache Solr is an open source enterprise search...
  stable/kafka-manager    	2.3.5        	1.3.3.22   	DEPRECATED - A tool for managing Apache Kafka. 
  下载
  helm pull bitnami/zookeeper
  解压查看
  [root@k8s-master01 helmzook]# ll
  总用量 40
  drwxr-xr-x 4 root root   128 10月 27 21:07 zookeeper
  -rw-r--r-- 1 root root 38256 10月 27 21:06 zookeeper-7.4.10.tgz
  
  修改values.yaml相应配置：副本数、auth(关闭授权认证）、持久化(需要的化可以结合pv)
  
  安装
  helm install -n 命名空间 安装的名称 . (点找到当前目录下的value.yaml)
  helm install -n public-service zookeeper  .
  [root@k8s-master01 zookeeper]# helm install -n public-service zokeeper .
  W1027 21:58:18.645141   31550 warnings.go:70] policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
  W1027 21:58:18.707399   31550 warnings.go:70] policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
  NAME: zokeeper
  LAST DEPLOYED: Wed Oct 27 21:58:18 2021
  NAMESPACE: public-service
  STATUS: deployed
  REVISION: 1
  TEST SUITE: None
  NOTES:
  CHART NAME: zookeeper
  CHART VERSION: 7.4.10
  APP VERSION: 3.7.0
  
  ** Please be patient while the chart is being deployed **
  
  ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:
  
      zokeeper-zookeeper.public-service.svc.cluster.local
  
  To connect to your ZooKeeper server run the following commands:
  
      export POD_NAME=$(kubectl get pods --namespace public-service -l "app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zokeeper,app.kubernetes.io/component=zookeeper" -o jsonpath="{.items[0].metadata.name}")
      kubectl exec -it $POD_NAME -- zkCli.sh
  
  To connect to your ZooKeeper server from outside the cluster execute the following commands:
  
      kubectl port-forward --namespace public-service svc/zokeeper-zookeeper 2181:2181 &
    zkCli.sh 127.0.0.1:2181
  ```
  
  查看zookeeper
  
  ```
  [root@k8s-master01 ~]# kubectl get pod -n public-service |grep zook
  zokeeper-zookeeper-0   1/1     Running   2          4d23h
  zokeeper-zookeeper-1   1/1     Running   2          4d23h
  zokeeper-zookeeper-2   1/1     Running   2          4d23h
  您在 /var/spool/mail/root 中有新邮件
  [root@k8s-master01 ~]# kubectl get svc -n public-service |grep zook
  zokeeper-zookeeper            ClusterIP   10.96.57.52    <none>        2181/TCP,2888/TCP,3888/TCP   4d23h
  zokeeper-zookeeper-headless   ClusterIP   None           <none>        2181/TCP,2888/TCP,3888/TCP   4d23h
  [root@k8s-master01 ~]# 
  ```
  
  
  
  Kafka
  
  ```
  安装kafka
  并且直接使用set命令进行直接修改(也可以修改value.yaml文件)
  
  helm install kafka bitnami/kafka --set zookeeper.enabled=false --set replicaCount=3 --set externalZookeeper.servers=zokeeper-zookeeper --set persistence.enabled=false -n public-service
  查看
  [root@k8s-master01 ~]# kubectl get pod -n public-service
  NAME                   READY   STATUS    RESTARTS   AGE
  kafka-0                0/1     Running   0          4s
  kafka-1                1/1     Running   0          18s
  kafka-2                1/1     Running   0          39s
  zokeeper-zookeeper-0   1/1     Running   2          4d23h
  zokeeper-zookeeper-1   1/1     Running   2          4d23h
  zokeeper-zookeeper-2   1/1     Running   2          4d23h
  
  ```
  
  常用一些命令
  
  ```
  helm list -n public-service  #列出命名空间下安装的应用以及版本
  [root@k8s-master01 ~]# helm list -n public-service
  NAME    	NAMESPACE     	REVISION	UPDATED                                	STATUS  	CHART           	APP VERSION
  kafka   	public-service	3       	2021-11-01 21:09:41.922779234 +0800 CST	deployed	kafka-14.2.4    	2.8.1      
  zokeeper	public-service	1       	2021-10-27 21:58:18.381635723 +0800 CST	deployed	zookeeper-7.4.10	3.7.0    
  查看value的值，这次是直接修改的，可以直接查看到
  [root@k8s-master01 ~]# helm get values kafka -n public-service
  USER-SUPPLIED VALUES:
  externalZookeeper:
    servers: zokeeper-zookeeper
  persistence:
    enabled: false
  replicaCount: 3
  zookeeper:
    enabled: false
    
    如果刚才执行错了，或者想编辑修改这样
  helm upgrade kafka bitnami/kafka --set zookeeper.enable ...
    
  ```
  
  kafka集群验证
  
  ```
  创建客户端
  kubectl run kafka-client --restart='Never' --image docker.io/bitnami/kafka:2.8.0-debian-10-r30 --namespace public-service --command -- sleep infinity
  [root@k8s-master01 ~]# kubectl get pod -n public-service -owide|grep client
  kafka-client           1/1     Running   0          90s     172.25.244.223   k8s-master01   <none>           <none>
  ```
  
  生产者
  
  ```
  kubectl exec --tty -i kafka-client --namespace public-service -- bash
  进入后执行
  生产者:
          kafka-console-producer.sh \
              --broker-list kafka-0.kafka-headless.public-service.svc.cluster.local:9092,kafka-1.kafka-headless.public-service.svc.cluster.local:9092,kafka-2.kafka-headless.public-service.svc.cluster.local:9092 \
              --topic test
  
  ```
  
  消费者
  
  ```
  在切换一个终端执行
  kubectl exec --tty -i kafka-client --namespace public-service -- bash
  消费者:
          kafka-console-consumer.sh \
              --bootstrap-server kafka.public-service.svc.cluster.local:9092 \
              --topic test \
              --from-beginning
  ```
  
  删除卸载包
  
  ```
  [root@k8s-master01 ~]# helm list -A -n public-service   #先查看
  NAME         	NAMESPACE     	REVISION	UPDATED                                	STATUS  	CHART              	APP VERSION
  ingress-nginx	ingress-nginx 	1       	2021-09-05 21:19:53.665808401 +0800 CST	deployed	ingress-nginx-4.0.1	1.0.0      
  kafka        	public-service	3       	2021-11-01 21:09:41.922779234 +0800 CST	deployed	kafka-14.2.4       	2.8.1      
  zokeeper     	public-service	1       	2021-10-27 21:58:18.381635723 +0800 CST	deployed	zookeeper-7.4.10   	3.7.0  
  卸载删除
  [root@k8s-master01 ~]# helm delete zokeeper -n public-service
  W1101 21:28:34.914776    5180 warnings.go:70] policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
  release "zokeeper" uninstalled
  ```
  
  