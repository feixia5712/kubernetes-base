## 准入控制资源配额

### 资源配额resourcequota

#### 简介

```
资源配额，通过 ResourceQuota 对象来定义，对每个命名空间的资源消耗总量提供限制。 它可以限制命名空间中某种类型的对象的总数目上限，也可以限制命令空间中的 Pod 可以使用的计算资源的总上限。主要是针对命名空间的限制,namespace下的资源限制
```
requests跟limit区别：https://www.cnblogs.com/robinunix/articles/11413059.html
requests是容器启动申请得最小资源
#### 常用限制资源列表

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-test
  labels:
    app: resourcequota
spec:
  hard:
    pods: 50
    requests.cpu: 0.5
    requests.memory: 512Mi
    limits.cpu: 5
    limits.memory: 16Gi
    configmaps: 20
    requests.storage: 40Gi
    persistentvolumeclaims: 20
    replicationcontrollers: 20
    secrets: 20
    services: 50
    services.loadbalancers: "2"
    services.nodeports: "10"
```

详解

```
pods：限制最多启动Pod的个数
requests.cpu：限制最高CPU请求数
requests.memory：限制最高内存的请求数
limits.cpu：限制最高CPU的limit上限
limits.memory：限制最高内存的limit上限
```

#### 简单案例演示

```
先创建一个命名空间
kubectl create ns rq-test
[root@k8s-master01 quota]# cat resourcequota.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-test
  labels:
    app: resourcequota
spec:
  hard:
    pods: 50
    #requests.cpu: 0.5
    #requests.memory: 512Mi
    #limits.cpu: 5
    #limits.memory: 16Gi
    configmaps: 2
    #requests.storage: 40Gi
    #persistentvolumeclaims: 20
    #replicationcontrollers: 20
    #secrets: 20
    #services: 50
    #services.loadbalancers: "2"
    #services.nodeports: "10"
    
只验证configmap看效果
查看现在资源状态
[root@k8s-master01 quota]# kubectl get resourcequota -n rq-test
NAME            AGE   REQUEST                       LIMIT
resource-test   52m   configmaps: 1/2, pods: 0/50   
默认有一个cm，还能创建一个
kubectl create configmap my-config --from-literal=Password=123 --from-literal=Name=alex -n rq-test
```

查看

```
[root@k8s-master01 quota]# kubectl get cm -n rq-test
NAME               DATA   AGE
kube-root-ca.crt   1      63m
my-config          2      20s
[root@k8s-master01 quota]# kubectl get resourcequota -n rq-test
NAME            AGE   REQUEST                       LIMIT
resource-test   55m   configmaps: 2/2, pods: 0/50   
[root@k8s-master01 quota]# 
cm已经创建满了
```

再次创建报错,提示已经满了

```
[root@k8s-master01 quota]# kubectl create configmap my-config01 --from-literal=Password=123 --from-literal=Name=alex -n rq-test
error: failed to create configmap: configmaps "my-config01" is forbidden: exceeded quota: resource-test, requested: configmaps=1, used: configmaps=2, limited: configmaps=2
您在 /var/spool/mail/root 中有新邮件
[root@k8s-master01 quota]# 

```

### LimitRange

```
 LimitRange 是在命名空间内限制资源分配（给多个 Pod 或 Container）的策略对象
 一个 LimitRange（限制范围） 对象提供的限制能够做到：
在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制。
在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制。
在一个命名空间中实施对一种资源的申请值和限制值的比值的控制。
设置一个命名空间中对计算资源的默认申请/限制值，并且自动的在运行时注入到多个 Container 中。
参考官网配置：https://kubernetes.io/zh/docs/concepts/policy/limit-range/
```

**LimitRange 的验证仅在 Pod 准入阶段进行，不对正在运行的 Pod 进行验证**

#### 案例分析

#### 配置默认资源限制

```
[root@k8s-master01 limitrange]# cat limitcpu.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container    
```

创建

```
kubectl create -f limitcpu.yaml -n limitrange
[root@k8s-master01 limitrange]# kubectl get -f limitcpu.yaml -n limitrange
NAME              CREATED AT
cpu-limit-range   2021-10-19T03:30:39Z
[root@k8s-master01 limitrange]# kubectl get -f limitcpu.yaml -n limitrange -oyaml
apiVersion: v1
kind: LimitRange
metadata:
  creationTimestamp: "2021-10-19T03:30:39Z"
  name: cpu-limit-range
  namespace: limitrange
  resourceVersion: "9317587"
  uid: 9400f5de-bbe7-405b-b8fb-8f1977767015
spec:
  limits:
  - default:
      cpu: "1"
      memory: 512M
    defaultRequest:
      cpu: 500m
      memory: 256M
    type: Container
    
 default默认limit配置限制一个cpu     defaultRequest默认是request请求500 millicpus的 CPU 请求
```

创建容器

```
[root@k8s-master01 limitrange]# cat nginx.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx
[root@k8s-master01 limitrange]# kubectl create nginx.yaml -n limitrange
[root@k8s-master01 limitrange]# kubectl create -f nginx.yaml -n limitrange
pod/default-cpu-demo created
[root@k8s-master01 limitrange]# kubectl get pod -n limit^C
[root@k8s-master01 limitrange]# kubectl get -f nginx.yaml -n limitrange
NAME               READY   STATUS              RESTARTS   AGE
default-cpu-demo   0/1     ContainerCreating   0          20s
[root@k8s-master01 limitrange]# kubectl get -f nginx.yaml -n limitrange -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container
      default-cpu-demo-ctr; cpu, memory limit for container default-cpu-demo-ctr'
  creationTimestamp: "2021-10-19T03:39:25Z"
  name: default-cpu-demo
  namespace: limitrange
  resourceVersion: "9318243"
  uid: 44e6864d-2845-4e85-88a8-84cbdc4d9e18
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-ctr
    resources:  #自动填写上设置得默认值
      limits:
        cpu: "1"
        memory: 512M
      requests:
        cpu: 500m
        memory: 256M
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
```

#### 为配置 资源最小和最大约束

```
[root@k8s-master01 limitrange]# cat mlimit.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
spec:
  limits:
  - max:
      cpu: "800m"
      memory: "1G"
    min:
      cpu: "200m"
      memory: "200M"
    type: Container
kubectl create -f mlimit.yaml -n limitrange  创建
```

查看

```
[root@k8s-master01 limitrange]# kubectl get limitrange -n limitrange -oyaml
apiVersion: v1
items:
- apiVersion: v1
  kind: LimitRange
  metadata:
    creationTimestamp: "2021-10-19T06:30:59Z"
    name: cpu-min-max-demo-lr
    namespace: limitrange
    resourceVersion: "9343077"
    uid: 3b2a89d1-b7ce-4a02-aec2-a18e6addb592
  spec:
    limits:
    - default:
        cpu: 800m
        memory: 1G
      defaultRequest:
        cpu: 800m
        memory: 1G
      max:
        cpu: 800m
        memory: 1G
      min:
        cpu: 200m
        memory: 200M
      type: Container
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
```

**输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。**

现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤：

- 如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。
- 核查容器声明的 CPU 请求确保其大于或者等于 200 millicpu。
- 核查容器声明的 CPU 限制确保其小于或者等于 800 millicpu。

合理范围创建

```
[root@k8s-master01 limitrange]# cat mnginx.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo
spec:
  containers:
  - name: constraints-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        cpu: "800m"
        memory: "500M"
      requests:
        cpu: "500m"
        memory: "500M"
kubectl create -f mnginx.yaml -n limitrange   在合理范围内，可以创建成功
```

修改下不再范围内

```
内存改大
[root@k8s-master01 limitrange]# cat mnginx.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: constraints-cpu-demo
spec:
  containers:
  - name: constraints-cpu-demo-ctr
    image: nginx
    resources:
      limits:
        cpu: "800m"
        memory: "2G"
      requests:
        cpu: "500m"
[root@k8s-master01 limitrange]# kubectl create -f mnginx.yaml -n limitrange
Error from server (Forbidden): error when creating "mnginx.yaml": pods "constraints-cpu-demo" is forbidden: maximum memory usage per Container is 1G, but limit is 2G  #提示报错
```

不指定任何参数得话，会获取默认配置

```
[root@k8s-master01 limitrange]# cat nginx.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx
查看
[root@k8s-master01 limitrange]# kubectl create -f mnginx.yaml -n limitrange
pod/constraints-cpu-demo created
[root@k8s-master01 limitrange]# kubectl get pod -n limitrange -oyaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for
        container constraints-cpu-demo-ctr; cpu, memory limit for container constraints-cpu-demo-ctr'
    creationTimestamp: "2021-10-19T06:42:35Z"
    name: constraints-cpu-demo
    namespace: limitrange
    resourceVersion: "9344781"
    uid: 5a4253a7-8413-42b1-bc9b-8e9b1216909f
  spec:
    containers:
    - image: nginx
      imagePullPolicy: Always
      name: constraints-cpu-demo-ctr
      resources:  #读取到了默认配置
        limits:
          cpu: 800m
          memory: 1G
        requests:
          cpu: 800m
          memory: 1G
      terminationMessagePath: /dev/termination-log
```

#### 限制存储消耗

```
将 LimitRange 添加到名字空间会为存储请求大小强制设置最小值和最大值。 存储是通过 PersistentVolumeClaim 来发起请求的。 执行限制范围控制的准入控制器会拒绝任何高于或低于管理员所设阈值的 PVC
[root@k8s-master01 limitrange]# cat mpv.yaml 
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 2Gi
    min:
      storage: 1Gi
max：最大PVC的空间
min：最小PVC的空间
```

创建应用pvc申请资源

```
[root@k8s-master01 limitrange]# cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi  #大于规定得
  storageClassName: slow
  
[root@k8s-master01 limitrange]# kubectl create -f pvc.yaml -n limitrange  #报错
Error from server (Forbidden): error when creating "pvc.yaml": persistentvolumeclaims "myclaim" is forbidden: maximum storage usage per PersistentVolumeClaim is 2Gi, but request is 8Gi
```

### Qos服务质量

```
Guaranteed：最高服务质量，当宿主机内存不够时，会先kill掉QoS为BestEffort和Burstable的Pod，如果内存还是不够，才会kill掉QoS为Guaranteed，该级别Pod的资源占用量一般比较明确，即requests的cpu和memory和limits的cpu和memory配置的一致。

Burstable： 服务质量低于Guaranteed，当宿主机内存不够时，会先kill掉QoS为BestEffort的Pod，如果内存还是不够之后就会kill掉QoS级别为Burstable的Pod，用来保证QoS质量为Guaranteed的Pod，该级别Pod一般知道最小资源使用量，但是当机器资源充足时，还是想尽可能的使用更多的资源，即limits字段的cpu和memory大于requests的cpu和memory的配置。

BestEffort：尽力而为，当宿主机内存不够时，首先kill的就是该QoS的Pod，用以保证Burstable和Guaranteed级别的Pod正常运行。

```

#### 实现QoS为Guaranteed的Pod

```
kubectl create ns qos-example
[root@k8s-master01 qos]# cat g.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        cpu: 1
        memory: "200Mi"
      requests:
        cpu: 1
        memory: "200Mi"     
```

创建查看

```
[root@k8s-master01 qos]# cat g.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        cpu: 1
        memory: "200Mi"
      requests:
        cpu: 1
        memory: "200Mi"
[root@k8s-master01 qos]# kubectl create -f g.yaml 
pod/qos-demo created
[root@k8s-master01 qos]# kubectl get -f g.yaml -oyaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-10-19T13:44:40Z"
  name: qos-demo
  namespace: qos-example
  resourceVersion: "9405963"
  uid: 40bd4928-672b-47e8-b761-0cad9be8a8c1
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr
    resources:
      limits:
        cpu: "1"    #保持limit跟request资源一样
        memory: 200Mi
      requests:
        cpu: "1"
        memory: 200Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-fr5kq
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-node02
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-fr5kq
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:44:40Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:44:58Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:44:58Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:44:40Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://10cb601f5ba6c79adec611c0d4e60e44b2c74d2aa62a44a6ef06745c28a0b3f1
    image: nginx:latest
    imageID: docker-pullable://nginx@sha256:644a70516a26004c97d0d85c7fe1d0c3a67ea8ab7ddf4aff193d9f301670cf36
    lastState: {}
    name: qos-demo-ctr
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2021-10-19T13:44:58Z"
  hostIP: 192.168.10.184
  phase: Running
  podIP: 172.27.14.229
  podIPs:
  - ip: 172.27.14.229
  qosClass: Guaranteed      #qos为最高服务质量
  startTime: "2021-10-19T13:44:40Z"
  
  
Pod中的每个容器必须指定limits.memory和requests.memory，并且两者需要相等；
Pod中的每个容器必须指定limits.cpu和limits.memory，并且两者需要相等。

```

#### QoS为Burstable的Pod

```
[root@k8s-master01 qos]# cat b.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
[root@k8s-master01 qos]# kubectl create -f b.yaml 
pod/qos-demo-2 created
[root@k8s-master01 qos]# kubectl get -f b.yaml -oyaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-10-19T13:48:00Z"
  name: qos-demo-2
  namespace: qos-example
  resourceVersion: "9406411"
  uid: a154fb85-8b34-412c-8498-0a9dd3b4c5e5
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-2-ctr
    resources:
      limits:
        memory: 200Mi
      requests:
        memory: 100Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-g278g
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-master01
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-g278g
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:48:00Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:48:00Z"
    message: 'containers with unready status: [qos-demo-2-ctr]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:48:00Z"
    message: 'containers with unready status: [qos-demo-2-ctr]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:48:00Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - image: nginx
    imageID: ""
    lastState: {}
    name: qos-demo-2-ctr
    ready: false
    restartCount: 0
    started: false
    state:
      waiting:
        reason: ContainerCreating
  hostIP: 192.168.10.180
  phase: Pending
  qosClass: Burstable    #qos值
  startTime: "2021-10-19T13:48:00Z"

```

```
Pod不符合Guaranteed的配置要求；
Pod中至少有一个容器配置了requests.cpu或requests.memory。
```

#### QoS为BestEffort的Pod

```
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-3-ctr
    image: nginx
    
不设置resources参数
```

演示如下

```
[root@k8s-master01 qos]# kubectl create -f c.yaml 
pod/qos-demo-3 created
[root@k8s-master01 qos]# kubectl get -f c.yaml -oyaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-10-19T13:52:02Z"
  name: qos-demo-3
  namespace: qos-example
  resourceVersion: "9407015"
  uid: 00ab59e3-fc5d-4800-8dc2-6555fbcda6f2
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-3-ctr
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-p6t8m
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: k8s-master01
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-p6t8m
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:52:02Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:52:02Z"
    message: 'containers with unready status: [qos-demo-3-ctr]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:52:02Z"
    message: 'containers with unready status: [qos-demo-3-ctr]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2021-10-19T13:52:02Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - image: nginx
    imageID: ""
    lastState: {}
    name: qos-demo-3-ctr
    ready: false
    restartCount: 0
    started: false
    state:
      waiting:
        reason: ContainerCreating
  hostIP: 192.168.10.180
  phase: Pending
  qosClass: BestEffort
  startTime: "2021-10-19T13:52:02Z"
```

参考Kubernetes 的 Limit 和 Request:https://zhuanlan.zhihu.com/p/114765307
